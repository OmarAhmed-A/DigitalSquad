{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import gym\n",
    "sys.path.append('c://Users/omara/Desktop/VSCODE/DigitalSquad/gym-maze/')\n",
    "import gym_maze\n",
    "from gym_maze.envs.maze_manager import MazeManager\n",
    "from riddle_solvers import *\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DQN hyperparameters\n",
    "# BUFFER_SIZE = 100000\n",
    "# BATCH_SIZE = 32\n",
    "# GAMMA = 0.99\n",
    "# EPSILON = 1.0\n",
    "# EPSILON_DECAY = 0.9999\n",
    "# EPSILON_MIN = 0.01\n",
    "# LEARNING_RATE = 0.001\n",
    "# UPDATE_FREQ = 1000\n",
    "\n",
    "# # Define the Q-network\n",
    "# def create_q_network(input_shape, num_actions):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(32, input_shape=input_shape, activation='relu'))\n",
    "#     model.add(Dense(32, activation='relu'))\n",
    "#     model.add(Dense(num_actions, activation=None))\n",
    "#     model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
    "#     return model\n",
    "\n",
    "# # Replay buffer for experience replay\n",
    "# class ReplayBuffer:\n",
    "#     def __init__(self):\n",
    "#         self.buffer = []\n",
    "#         self.buffer_size = BUFFER_SIZE\n",
    "    \n",
    "#     def add(self, experience):\n",
    "#         if len(self.buffer) >= self.buffer_size:\n",
    "#             self.buffer.pop(0)\n",
    "#         self.buffer.append(experience)\n",
    "    \n",
    "#     def sample(self, batch_size):\n",
    "#         if len(self.buffer) < batch_size:\n",
    "#             return None\n",
    "#         samples = random.sample(self.buffer, batch_size)\n",
    "#         states, actions, rewards, next_states, dones = zip(*samples)\n",
    "#         return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "# # DQN agent\n",
    "# class DQNAgent:\n",
    "#     def __init__(self, state_shape, num_actions):\n",
    "#         self.q_network = create_q_network(state_shape, num_actions)\n",
    "#         self.target_network = create_q_network(state_shape, num_actions)\n",
    "#         self.buffer = ReplayBuffer()\n",
    "#         self.num_actions = num_actions\n",
    "#         self.step_count = 0\n",
    "    \n",
    "#     def select_action(self, state, epsilon):\n",
    "#         if np.random.random() < epsilon:\n",
    "#             action = np.random.choice(self.num_actions)\n",
    "#         else:\n",
    "#             q_values = self.q_network.predict(np.array([state]))[0]\n",
    "#             action = np.argmax(q_values)\n",
    "#         return action\n",
    "    \n",
    "#     def update_networks(self):\n",
    "#         weights = self.q_network.get_weights()\n",
    "#         self.target_network.set_weights(weights)\n",
    "    \n",
    "#     def train(self, batch_size):\n",
    "#         if len(self.buffer.buffer) < batch_size:\n",
    "#             return\n",
    "#         states, actions, rewards, next_states, dones = self.buffer.sample(batch_size)\n",
    "#         q_values_next = self.target_network.predict(next_states)\n",
    "#         targets = rewards + (1 - dones) * GAMMA * np.amax(q_values_next, axis=1)\n",
    "#         q_values = self.q_network.predict(states)\n",
    "#         q_values[np.arange(batch_size), actions] = targets\n",
    "#         self.q_network.fit(states, q_values, verbose=0)\n",
    "    \n",
    "#     def remember(self, state, action, reward, next_state, done):\n",
    "#         self.buffer.add((state, action, reward, next_state, done))\n",
    "    \n",
    "#     def decay_epsilon(self):\n",
    "#         epsilon = max(EPSILON_MIN, EPSILON_DECAY * self.step_count)\n",
    "#         self.step_count += 1\n",
    "#         return epsilon\n",
    "    \n",
    "#     def save_model(self, filename):\n",
    "#         self.q_network.save(filename)\n",
    "    \n",
    "#     def load_model(self, filename):\n",
    "#         self.q_network = tf.keras.models.load_model(filename)\n",
    "#         self.target_network = create_q_network(self.q_network.input_shape[1:], self.num_actions)\n",
    "#         self.update_networks()\n",
    "\n",
    "# # Train the agent\n",
    "# def train_agent(env, agent, num_episodes, batch_size):\n",
    "#     scores = []\n",
    "#     for episode in range(num_episodes):\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#         score = 0\n",
    "#         while not done:\n",
    "#             action = agent.select_action(state, agent.decay_epsilon())\n",
    "#             next_state, reward, done, info = env.step(action)\n",
    "#             agent.remember(state, action, reward, next_state, done)\n",
    "#             agent.train(batch_size)\n",
    "#             state = next_state\n",
    "#             score += reward\n",
    "#         scores.append(score)\n",
    "#         if episode % UPDATE_FREQ == 0:\n",
    "#             agent.update_networks()\n",
    "#             print('Episode: {}, Score: {}'.format(episode, score))\n",
    "#     return scores\n",
    "\n",
    "# # Test the agent\n",
    "# def test_agent(env, agent, num_episodes):\n",
    "#     scores = []\n",
    "#     for episode in range(num_episodes):\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#         score = 0\n",
    "#         while not done:\n",
    "#             action = agent.select_action(state, 0)\n",
    "#             next_state, reward, done, info = env.step(action)\n",
    "#             state = next_state\n",
    "#             score += reward\n",
    "#         scores.append(score)\n",
    "#     return scores\n",
    "\n",
    "# # Create the environment\n",
    "# env = gym.make('maze-sample-10x10-v0')\n",
    "# env = MazeManager(env)\n",
    "\n",
    "# # Create the agent\n",
    "# agent = DQNAgent(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "# # Train the agent\n",
    "# scores = train_agent(env, agent, 10000, BATCH_SIZE)\n",
    "\n",
    "# # Test the agent\n",
    "# test_scores = test_agent(env, agent, 100)\n",
    "\n",
    "# # Save the model\n",
    "# agent.save_model('dqn_maze.h5')\n",
    "\n",
    "# # Plot the scores\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(scores)\n",
    "# plt.plot(test_scores)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN hyperparameters\n",
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.9999\n",
    "EPSILON_MIN = 0.01\n",
    "LEARNING_RATE = 0.001\n",
    "UPDATE_FREQ = 1000\n",
    "\n",
    "# Define the Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_shape[0], 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Replay buffer for experience replay\n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "# DQN agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, num_actions, BUFFER_SIZE):\n",
    "        self.q_network = QNetwork(state_shape, num_actions)\n",
    "        self.target_network = QNetwork(state_shape, num_actions)\n",
    "        self.buffer = ReplayBuffer(buffer_size=BUFFER_SIZE)\n",
    "        self.num_actions = num_actions\n",
    "        self.step_count = 0\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    def select_action(self, state, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            q_values = self.q_network(torch.FloatTensor([state])).detach().numpy()[0]\n",
    "            action = np.argmax(q_values)\n",
    "        return action\n",
    "    \n",
    "    def update_networks(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def train(self, buffer, batch_size):\n",
    "        # Sample a batch of transitions from replay buffer\n",
    "        states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "\n",
    "        # Convert arrays to tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool)\n",
    "\n",
    "        # Calculate TD targets\n",
    "        with torch.no_grad():\n",
    "            q_next = self.target_network(next_states)\n",
    "            q_next[dones] = 0.0\n",
    "            targets = rewards + self.gamma * torch.max(q_next, dim=1)[0]\n",
    "\n",
    "        # Calculate current Q-values and loss\n",
    "        q_values = self.policy_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        loss = self.loss_fn(q_values, targets)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        self.soft_update_target_network(self.target_network, self.policy_network, self.tau)\n",
    "\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.buffer.add((state, action, reward, next_state, done))\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        epsilon = max(EPSILON_MIN, EPSILON_DECAY * self.step_count)\n",
    "        self.step_count += 1\n",
    "        return epsilon\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        torch.save(self.q_network.state_dict(), filename)\n",
    "    \n",
    "    def load_model(self, filename):\n",
    "        self.q_network.load_state_dict(torch.load(filename))\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(start, end):\n",
    "    return abs(start[0] - end[0]) + abs(start[1] - end[1])\n",
    "\n",
    "def get_reward(obv):\n",
    "    \n",
    "        \n",
    "    return 0 - manhattan_distance(obv[0], np.array((1, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rescue items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-377431d6d038>:41: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  states = np.array(states)\n",
      "<ipython-input-43-377431d6d038>:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  next_states = np.array(next_states)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-7fa832240c69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# Train agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mdqn_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-377431d6d038>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, buffer, batch_size)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# Convert arrays to tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "# Initialize maze manager and environment\n",
    "sample_maze = np.load(\"hackathon_sample.npy\")\n",
    "agent_id = \"9\" # add your agent id here\n",
    "manager = MazeManager()\n",
    "manager.init_maze(agent_id, maze_cells=sample_maze)\n",
    "env = manager.maze_map[agent_id]\n",
    "\n",
    "# Initialize DQN agent and replay buffer\n",
    "state_shape = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dqn_agent = DQNAgent(state_shape, num_actions, BUFFER_SIZE=10000)\n",
    "memory = dqn_agent.buffer\n",
    "\n",
    "# Training loop\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "BATCH_SIZE = 32\n",
    "UPDATE_FREQ = 10\n",
    "scores = []\n",
    "for episode in range(500):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        # Choose action\n",
    "        action = dqn_agent.select_action(state, EPSILON)\n",
    "\n",
    "        # Take action\n",
    "        observation, _, done, _, info = env.step(action)\n",
    "        reward = get_reward(observation)\n",
    "\n",
    "        score += reward\n",
    "\n",
    "        # Store transition in replay buffer\n",
    "        next_state = observation\n",
    "        memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Train agent\n",
    "        if len(memory) >= BATCH_SIZE:\n",
    "            dqn_agent.train(memory, BATCH_SIZE)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay exploration rate\n",
    "    EPSILON = max(EPSILON_MIN, EPSILON_DECAY * EPSILON)\n",
    "\n",
    "    # Update target network\n",
    "    if episode % UPDATE_FREQ == 0:\n",
    "        dqn_agent.update_target_network()\n",
    "\n",
    "    # Print episode info\n",
    "    scores.append(score)\n",
    "    print(\"Episode\", episode, \"completed with reward:\", score)\n",
    "\n",
    "# Save trained model\n",
    "dqn_agent.save_model(\"trained_dqn.pt\")\n",
    "\n",
    "# Close environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omara\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\omara\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\omara\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'list'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rescue items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omara\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:137: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'list'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\omara\\anaconda3\\lib\\site-packages\\gym\\spaces\\box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  logger.warn(\"Casting input x to numpy array.\")\n",
      "c:\\Users\\omara\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "c:\\Users\\omara\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:252: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'NoneType'>\u001b[0m\n",
      "  logger.warn(\n",
      "<ipython-input-8-f3a8a62f9190>:36: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-14aa432374de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# Train agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mdqn_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-f3a8a62f9190>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mq_values_next\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mGAMMA\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values_next\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\omara\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\omara\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "# # Initialize maze manager and environment\n",
    "# sample_maze = np.load(\"hackathon_sample.npy\")\n",
    "# agent_id = \"9\" # add your agent id here\n",
    "# manager = MazeManager()\n",
    "# manager.init_maze(agent_id, maze_cells=sample_maze)\n",
    "# env = manager.maze_map[agent_id]\n",
    "\n",
    "# # Initialize DQN agent and replay buffer\n",
    "# state_shape = env.observation_space.shape\n",
    "# num_actions = env.action_space.n\n",
    "# dqn_agent = DQNAgent(state_shape, num_actions)\n",
    "# memory = dqn_agent.buffer\n",
    "\n",
    "# # Training loop\n",
    "# scores = []\n",
    "# for episode in range(500):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     score = 0\n",
    "#     while not done:\n",
    "#         # Choose action\n",
    "#         action = dqn_agent.select_action(state, EPSILON)\n",
    "\n",
    "#         # Take action\n",
    "#         observation, _, done, _, info = env.step(action)\n",
    "#         reward = get_reward(observation)\n",
    "\n",
    "#         score += reward\n",
    "\n",
    "#         # Store transition in replay buffer\n",
    "#         dqn_agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "#         # Update state\n",
    "#         state = next_state\n",
    "\n",
    "#         # Train agent\n",
    "#         dqn_agent.train(BATCH_SIZE)\n",
    "\n",
    "#         if done:\n",
    "#             break\n",
    "\n",
    "#     # Decay exploration rate\n",
    "#     EPSILON = max(EPSILON_MIN, EPSILON_DECAY * EPSILON)\n",
    "\n",
    "#     # Update target network\n",
    "#     if episode % UPDATE_FREQ == 0:\n",
    "#         dqn_agent.update_networks()\n",
    "\n",
    "#     # Print episode info\n",
    "#     scores.append(score)\n",
    "#     print(\"Episode\", episode, \"completed with reward:\", score)\n",
    "\n",
    "# # Save trained model\n",
    "# dqn_agent.save_model(\"trained_dqn.h5\")\n",
    "\n",
    "# # Close environment\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test trained model\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    # Choose action\n",
    "    action = dqn_agent.act(state, explore=False)\n",
    "\n",
    "    # Take action\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    # Update state\n",
    "    state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "    # Render environment\n",
    "    env.render()\n",
    "\n",
    "print(\"Test completed with reward:\", total_reward)\n",
    "\n",
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# from collections import deque\n",
    "\n",
    "# class DQNAgent:\n",
    "#     def __init__(self, state_size, action_size, seed):\n",
    "#         self.state_size = state_size\n",
    "#         self.action_size = action_size\n",
    "#         self.seed = seed\n",
    "        \n",
    "#         # Hyperparameters\n",
    "#         self.buffer_size = int(1e5)\n",
    "#         self.batch_size = 64\n",
    "#         self.gamma = 0.99\n",
    "#         self.tau = 1e-3\n",
    "#         self.lr = 5e-4\n",
    "#         self.update_every = 4\n",
    "#         self.learn_step = 0\n",
    "        \n",
    "#         # Q-Network\n",
    "#         self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "#         self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "#         self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=self.lr)\n",
    "        \n",
    "#         # Replay memory\n",
    "#         self.memory = ReplayBuffer(action_size, self.buffer_size, self.batch_size, seed)\n",
    "        \n",
    "#     def step(self, state, action, reward, next_state, done):\n",
    "#         self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "#         self.learn_step += 1\n",
    "#         if self.learn_step % self.update_every == 0:\n",
    "#             if len(self.memory) > self.batch_size:\n",
    "#                 experiences = self.memory.sample()\n",
    "#                 self.learn(experiences, self.gamma)\n",
    "                \n",
    "#     def act(self, state, eps=0.):\n",
    "#         state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "#         self.qnetwork_local.eval()\n",
    "#         with torch.no_grad():\n",
    "#             action_values = self.qnetwork_local(state)\n",
    "#         self.qnetwork_local.train()\n",
    "        \n",
    "#         if random.random() > eps:\n",
    "#             return np.argmax(action_values.cpu().data.numpy())\n",
    "#         else:\n",
    "#             return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "#     def learn(self, experiences, gamma):\n",
    "#         states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "#         q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "#         q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n",
    "#         q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "#         loss = F.mse_loss(q_expected, q_targets)\n",
    "#         self.optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "        \n",
    "#         self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
    "        \n",
    "#     def soft_update(self, local_model, target_model, tau):\n",
    "#         for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "#             target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "\n",
    "# class QNetwork(nn.Module):\n",
    "#     def __init__(self, state_size, action_size, seed):\n",
    "#         super(QNetwork, self).__init__()\n",
    "#         self.seed = torch.manual_seed(seed)\n",
    "#         self.fc1 = nn.Linear(state_size, 64)\n",
    "#         self.fc2 = nn.Linear(64, 64)\n",
    "#         self.fc3 = nn.Linear(64, action_size)\n",
    "        \n",
    "#     def forward(self, state):\n",
    "#         x = F.relu(self.fc1(state))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         return self.fc3(x)\n",
    "    \n",
    "    \n",
    "# class ReplayBuffer:\n",
    "#     def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "#         self.action_size = action_size\n",
    "#         self.memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Step 1: Initialize environment\n",
    "# maze = Maze(width=10, height=10, complexity=0.2, density=0.2)\n",
    "# env = MazeEnv(maze)\n",
    "\n",
    "# # Step 2: Define DQN model architecture and create DQN agent\n",
    "# class DQN(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 64)\n",
    "#         self.fc2 = nn.Linear(64, 64)\n",
    "#         self.fc3 = nn.Linear(64, output_dim)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# class DQNAgent:\n",
    "#     def __init__(self, state_dim, action_dim, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.999, lr=0.001, batch_size=64, memory_size=10000):\n",
    "#         self.state_dim = state_dim\n",
    "#         self.action_dim = action_dim\n",
    "#         self.gamma = gamma\n",
    "#         self.epsilon = epsilon\n",
    "#         self.epsilon_min = epsilon_min\n",
    "#         self.epsilon_decay = epsilon_decay\n",
    "#         self.lr = lr\n",
    "#         self.batch_size = batch_size\n",
    "#         self.memory = []\n",
    "#         self.memory_size = memory_size\n",
    "#         self.model = DQN(state_dim, action_dim)\n",
    "#         self.target_model = DQN(state_dim, action_dim)\n",
    "#         self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "#         self.loss_fn = nn.MSELoss()\n",
    "    \n",
    "#     def act(self, state):\n",
    "#         if random.random() < self.epsilon:\n",
    "#             return random.randrange(self.action_dim)\n",
    "#         else:\n",
    "#             with torch.no_grad():\n",
    "#                 q_values = self.model(torch.tensor(state).float())\n",
    "#                 return torch.argmax(q_values).item()\n",
    "    \n",
    "#     def remember(self, state, action, reward, next_state, done):\n",
    "#         self.memory.append((state, action, reward, next_state, done))\n",
    "#         if len(self.memory) > self.memory_size:\n",
    "#             self.memory.pop(0)\n",
    "    \n",
    "#     def replay(self):\n",
    "#         if len(self.memory) < self.batch_size:\n",
    "#             return\n",
    "        \n",
    "#         batch = random.sample(self.memory, self.batch_size)\n",
    "#         states, actions, rewards, next_states, dones = zip(*batch)\n",
    "#         states = torch.tensor(states).float()\n",
    "#         actions = torch.tensor(actions).long()\n",
    "#         rewards = torch.tensor(rewards).float()\n",
    "#         next_states = torch.tensor(next_states).float()\n",
    "#         dones = torch.tensor(dones).float()\n",
    "        \n",
    "#         q_values = self.model(states)\n",
    "#         next_q_values = self.target_model(next_states).detach()\n",
    "#         target_q_values = rewards + (1 - dones) * self.gamma * torch.max(next_q_values, dim=1)[0]\n",
    "#         target_q_values = target_q_values.unsqueeze(1)\n",
    "#         target\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
